{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SS7nfICxgkWu"
      },
      "source": [
        "# Natural Language Processing 2024 â€“ Ex. 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hq8U8nL8yfH6"
      },
      "source": [
        "**Add the names and ID of the submitting students here:**\n",
        "\n",
        "1. Yaniv gabay\n",
        "\n",
        "\n",
        "2. Sahar Asher\n",
        "\n",
        "\n",
        "3. Hadar Video"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLnhXF3ok9Cd"
      },
      "source": [
        "\n",
        "In this exercise we will perform the task of Sentiment analysis over the IMDB movie review dataset.\n",
        "\n",
        "The dataset has around 50K movie reviews with each review labeled as \"positive\" or \"negative\".\n",
        "\n",
        "Our goal is given the review we want to classifiy it as positive or negative, this task is also called \"Sentiment Analysis\"\n",
        "\n",
        "Below you can find a suggestion of the order things should be implemented, you can follow this or do it your own way.\n",
        "\n",
        "The exercise has several stages:\n",
        "\n",
        "1. Downloading and cleaning the data\n",
        "2. Running some basic analysis\n",
        "3. Training a Feed Forward network to perform the task using classification\n",
        "4. Training a Bi-Dir LSTM to perform the task\n",
        "5. Playing with paramters to see if we get better results\n",
        "\n",
        "Please sumbit the notebook after it's running stage. Grade will be given for clean code, with comments and explanations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: contractions in c:\\users\\yanivg\\appdata\\roaming\\python\\python311\\site-packages (0.1.73)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in c:\\users\\yanivg\\appdata\\roaming\\python\\python311\\site-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: anyascii in c:\\users\\yanivg\\appdata\\roaming\\python\\python311\\site-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
            "Requirement already satisfied: pyahocorasick in c:\\users\\yanivg\\appdata\\roaming\\python\\python311\\site-packages (from textsearch>=0.0.21->contractions) (2.0.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\yanivg\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\yanivg\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\yanivg\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "import re\n",
        "!pip install contractions\n",
        "import contractions\n",
        "\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QfMknbomReU"
      },
      "source": [
        "# Data download and cleaning\n",
        "\n",
        "1. Download the IMDB dataset.\n",
        "\n",
        "2. Clean the data:\n",
        "* Remove URLs, HTML tags and non-alphanumeric characters\n",
        "* Remove stop-words (use NLTK)\n",
        "* Lowercase the dataset\n",
        "* (Optional) Anything else you think can help...\n",
        "\n",
        "Show one example of a review before and after this cleaning (find a review which has at least one URL/HTML tag/Non-aplhanumeric characters)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "h9PUr7Mtk7s8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original: This is an example review with HTML <b>bold</b> tags and a URL: https://example.com\n",
            "Cleaned: example review html bold tags url\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review sentiment\n",
              "0  One of the other reviewers has mentioned that ...  positive\n",
              "1  A wonderful little production. <br /><br />The...  positive\n",
              "2  I thought this was a wonderful way to spend ti...  positive\n",
              "3  Basically there's a family where a little boy ...  negative\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cleaned text printing:\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>one reviewers mentioned watching DIGPLACHOLD o...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>wonderful little production filming technique ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>thought wonderful way spend time hot summer we...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>basically family little boy jake thinks zombie...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>petter mattei love time money visually stunnin...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review sentiment\n",
              "0  one reviewers mentioned watching DIGPLACHOLD o...  positive\n",
              "1  wonderful little production filming technique ...  positive\n",
              "2  thought wonderful way spend time hot summer we...  positive\n",
              "3  basically family little boy jake thinks zombie...  negative\n",
              "4  petter mattei love time money visually stunnin...  positive"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "35225    john boorman deliverance concerns four suburba...\n",
            "27817    seen gong show tv series like movie much knowi...\n",
            "12825    writer director reading imagine since work mus...\n",
            "36077    following movie represents pinnacle DIGPLACHOL...\n",
            "17751    first let say wasted halloween movie night wat...\n",
            "19221    radio good movie honestly never cry movies pre...\n",
            "19250    film works true trying say ignore dynamics plo...\n",
            "1035     ok original mean rich old geezer leaves estate...\n",
            "31022    one worst film come britain DIGPLACHOLDs tawdr...\n",
            "16516    great concept great cast pity time flesh story...\n",
            "Name: review, dtype: object\n"
          ]
        }
      ],
      "source": [
        "# we tried to spell correct the data but the results very not good\n",
        "# so we decided to not use it example \"muslims\" into \"musea\"\n",
        "digitplaceholder = 'DIGPLACHOLD'\n",
        "#this function will export the curr data into csv\n",
        "#so we can obeserve the data more carfully\n",
        "#if needed\n",
        "def export_curr_data_to_csv(num_of_question,df):\n",
        "    try:\n",
        "         output_file = \"curr_data\"+num_of_question+\".csv\"\n",
        "         df.to_csv(output_file, index=False)\n",
        "         return \n",
        "    except Exception as e:\n",
        "          print(\"Error loading data: {}\".format(e))\n",
        "          exit(1)\n",
        "\n",
        "def clean_text(text):\n",
        "    #1 Remove HTML tags\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    #2 Remove URLs\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    #3 Remove non-alphanumeric characters\n",
        "    text = re.sub(r'\\W', ' ', text)\n",
        "    #4 Convert to lowercase\n",
        "    text = text.lower()\n",
        "    #5 Remove stopwords\n",
        "    stop_words = set(stopwords.words('english')) \n",
        "    #maybe we need some stopwords, so decrease them from this list if needed\n",
        "    words = text.split()\n",
        "    text = ' '.join([word for word in words if word not in stop_words])\n",
        "    #6 turn Digits into Token\n",
        "    text = re.sub(r'\\d+', digitplaceholder, text)\n",
        "    #7 additional steps if needed\n",
        "\n",
        "    #maybe remove some special chars \n",
        "   \n",
        "    # will need to see how the next step handle this example\n",
        "   \n",
        "    #should we remove all stop words? prob not\n",
        "    #example not good - turns into \"good\"\n",
        "    #so we need to think of a solution, maybe decrease from \n",
        "    #the stop words list some of the words that are important\n",
        "    return text\n",
        "def load_text(filename):\n",
        "   file_path = filename  \n",
        "   #IMDB data set has header at 0 row\n",
        "   data = pd.read_csv(file_path, header=0, names=['review', 'sentiment'])\n",
        "   display(data.head())\n",
        "    # Inspect the data\n",
        "   \n",
        "    # Check for missing values\n",
        "   #print(data.isnull().sum())\n",
        "   #print(data.info())\n",
        "\n",
        "    # Apply the cleaning function to the review column\n",
        "   data['review'] = data['review'].apply(clean_text)\n",
        "   return data\n",
        "\n",
        "# Example usage\n",
        "example_review = \"This is an example review with HTML <b>bold</b> tags and a URL: https://example.com\"\n",
        "cleaned_review = clean_text(example_review)\n",
        "print(\"Original:\", example_review)\n",
        "print(\"Cleaned:\", cleaned_review)\n",
        "try:\n",
        "# Load the data\n",
        "    data = load_text('IMDB_Dataset.csv')\n",
        "    data_orig_copy = data.copy()\n",
        "    print(\"cleaned text printing:\\n\")\n",
        "    display(data.head())\n",
        "    print(data.sample(10)['review'])\n",
        "    export_curr_data_to_csv(\"1\",data)\n",
        "   # describe = data.describe()\n",
        "   # print(describe)\n",
        "   \n",
        "    \n",
        "except Exception as e:\n",
        "    print(\"Error loading data: {}\".format(e))\n",
        "    exit(1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIVRS_efnIWM"
      },
      "source": [
        "# Tokenization\n",
        "\n",
        "1. Tokenize the dataset (you can tokenize using spaces or use more robust methods from NLTK)\n",
        "2. (Optional) Lemmatize the text (you can use NLTK) this can improve results\n",
        "3. Lemmatize should be carfully be done, so we wont lose too much.\n",
        "4. Show an example of 3 sentences before and after this process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "AdlCQ--gnwYh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original: The cats are chasing the mice.\n",
            "Processed: The cat are chase the mouse . \n",
            "\n",
            "Original: He was running late for the meeting.\n",
            "Processed: He was run late for the meeting . \n",
            "\n",
            "Original: She's not enjoying the sunny weather.\n",
            "Processed: She is not enjoy the sunny weather . \n",
            "\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import wordnet\n",
        "from nltk.tag import pos_tag\n",
        "\n",
        "\n",
        "#we decided to use POS tagging\n",
        "# this prevent the lemmatize process from taking\n",
        "# words like was and convert them into \"wa\"\n",
        "\n",
        "#will make she's into she is\n",
        "def expand_text_contractions(text):\n",
        "    expanded_text = contractions.fix(text)\n",
        "    return expanded_text\n",
        "\n",
        "#need to hansle situations like she's where currently it goes into she 's . so the best solution \n",
        "#currently is to delete 's from the text\n",
        "auxiliary_verbs = {\"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"being\", \"been\", \"will\", \"shall\", \"would\", \"should\", \"can\", \"could\", \"may\", \"might\", \"must\", \"do\", \"does\", \"did\", \"have\", \"has\", \"had\"}\n",
        "\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    \"\"\"Map POS tag to the format accepted by WordNetLemmatizer.\"\"\"\n",
        "    tag_dict = {\n",
        "        'J': wordnet.ADJ,\n",
        "        'N': wordnet.NOUN,\n",
        "        'V': wordnet.VERB,\n",
        "        'R': wordnet.ADV\n",
        "    }\n",
        "    return tag_dict.get(treebank_tag[0].upper(), wordnet.NOUN)\n",
        "def tokenize_and_lemmatize(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = word_tokenize(text)\n",
        "    pos_tags = nltk.pos_tag(tokens)\n",
        "    #print(\"POS Tags:\", pos_tags)\n",
        "    lemmatized_tokens = []\n",
        "    for token, pos in pos_tags:\n",
        "        # Skip lemmatization for auxiliary verbs\n",
        "        #from our expereince we saw that the lemmatize function\n",
        "        #converts words like was into wa\n",
        "        #and in general didnt handle aux verbs well\n",
        "        if token in auxiliary_verbs: \n",
        "            \n",
        "            lemmatized_tokens.append(token)\n",
        "        else:\n",
        "            \n",
        "            lemma = lemmatizer.lemmatize(token, get_wordnet_pos(pos))\n",
        "            lemmatized_tokens.append(lemma)\n",
        "            \n",
        "    return ' '.join(lemmatized_tokens)\n",
        "\n",
        "\n",
        "\n",
        "# Example sentences\n",
        "example_sentences = [\n",
        "    \"The cats are chasing the mice.\",\n",
        "    \"He was running late for the meeting.\",\n",
        "    \"She's not enjoying the sunny weather.\"\n",
        "]\n",
        "\n",
        "\n",
        "# Process and display the examples\n",
        "for sentence in example_sentences:\n",
        "    expanded_sentence = expand_text_contractions(sentence)\n",
        "    processed = tokenize_and_lemmatize(expanded_sentence)\n",
        "    print(\"Original:\", sentence)\n",
        "    print(\"Processed:\", processed, \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uls-qLAcoT2Z"
      },
      "source": [
        "# Basic analysis\n",
        "\n",
        "Perfrom some analysis on the data\n",
        "1. Show the number percentage of negative/positive review (label balancing)\n",
        "2. Plot some statistics on the length of review (after our cleaning process)\n",
        "3. (Optional) show anything else you think is important"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jnpnbjXcoRJx"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                              review  \\\n",
            "0  one reviewers mentioned watching DIGPLACHOLD o...   \n",
            "1  wonderful little production filming technique ...   \n",
            "2  thought wonderful way spend time hot summer we...   \n",
            "3  basically family little boy jake thinks zombie...   \n",
            "4  petter mattei love time money visually stunnin...   \n",
            "\n",
            "                                    processed_review  \n",
            "0  one reviewer mention watch DIGPLACHOLD oz epis...  \n",
            "1  wonderful little production film technique una...  \n",
            "2  think wonderful way spend time hot summer week...  \n",
            "3  basically family little boy jake think zombie ...  \n",
            "4  petter mattei love time money visually stunnin...  \n"
          ]
        }
      ],
      "source": [
        "#beware long run time if not on google colab\n",
        "# 6-7 min run time (POS tagging takes time) \n",
        "def final_preprocess(review):\n",
        "    # First, expand contractions\n",
        "    expanded_review = expand_text_contractions(review)\n",
        "    # Then, tokenize and lemmatize the expanded review\n",
        "    processed_review = tokenize_and_lemmatize(expanded_review)\n",
        "    return processed_review\n",
        "\n",
        "# Apply the preprocess_review function to each review in the DataFrame\n",
        "#we created another col here, instead of rewriting the data.\n",
        "#we wanted to maybe play with both results, and see how good\n",
        "#tokenize_and_lemmatize and expand_text_contractions are.\n",
        "data['processed_review'] = data['review'].apply(final_preprocess)\n",
        "\n",
        "#data right now is :\n",
        "#1.review |2. sentiment |3. processed_review\n",
        "#:\n",
        "#1.review - is after preprocessing\n",
        "#2.sentiment - is the original data (havent changed)\n",
        "#3.processed_review - is after clean and final preprocess so proccesed.\n",
        "\n",
        "# Display the first few rows to verify the changes\n",
        "print(data[['review', 'processed_review']].head())\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sentiment\n",
            "positive    50.0\n",
            "negative    50.0\n",
            "Name: proportion, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "##### label balancing\n",
        "label_counts = data_orig_copy['sentiment'].value_counts(normalize=True) * 100\n",
        "print(label_counts)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Statistics on the length of cleaned reviews:\n",
            "count    50000.000000\n",
            "mean       794.851240\n",
            "std        613.754239\n",
            "min         17.000000\n",
            "25%        416.000000\n",
            "50%        584.500000\n",
            "75%        966.000000\n",
            "max       8832.000000\n",
            "Name: review_length, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# Calculating the length of each cleaned review\n",
        "data['review_length'] = data['processed_review'].apply(len)\n",
        "\n",
        "# Displaying basic statistics\n",
        "print(\"Statistics on the length of cleaned reviews:\")\n",
        "print(data['review_length'].describe())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-b0Fu9vqocP"
      },
      "source": [
        "# Preparing the dataset for training\n",
        "we can also use glove or previously used models as the first layer\n",
        "1. Choose your vocabulary size K (should be between 1000 and 3000)\n",
        "2. Find the top K frequent words in your database\n",
        "3. Create word indexes like we did in class, for any word not in your top K  words replace with \\<UNK\\>. Remember to add an index for the \\<PAD\\> token.\n",
        "4. Create a new dataset with indexes instead of words later to be used for training\n",
        "5. Convert your labels to numeric representation (that your network can deal with).\n",
        "\n",
        "Split the dataset to 80% traind and 20% test, remember to keep the balance between labels!\n",
        "we need to make sure, we still have enough labels on both sides\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ZawXFmEHqnvA"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UokkSJLp5VQ"
      },
      "source": [
        "# Training a feed forward neural network\n",
        "\n",
        "For simplicity we would take only reviews with 500 words (after tokenization) or less.\n",
        "For this part we would train a neural network that gets the full review as one input (like we had in our NER example in class) and outputs the label (positive or negative).\n",
        "Remember that you need to PAD the words so all reviews will have the same length.\n",
        "\n",
        "For this section please try at least 3 variants of different network and show if the results change, you can choose from the following:\n",
        "1. Adding hidden layers to the network\n",
        "2. Running with and without Dropout\n",
        "3. Trying different optimizers\n",
        "\n",
        "(Optional) Try to use the Glove embedding: Create an embedding layer in your PyTorch model using the loaded GloVe embeddings. You will initialize the weights of the embedding layer with the GloVe embeddings.\n",
        "\n",
        "For each option:\n",
        "\n",
        "* Plot the train and test error during training, does your network overfit?\n",
        "\n",
        "* Plot the final results of the network, including accuracy and confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "NFtHmXUKp4hI"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoY1QnIdtCk3"
      },
      "source": [
        "# Training a BiDir LSTM neural network\n",
        "\n",
        "Now do the same as the prvious section with a bi-directional LSTM.\n",
        "\n",
        "Remember that the output of the LSTM should be connected to a small feed forward network to perform the actual classification.\n",
        "\n",
        "Here again you can play with number of layers and the LSTM or the small network of the output. Show only the best result you got.\n",
        "\n",
        "* Plot the train and test error during training, does your network overfit?\n",
        "\n",
        "* Plot the final results of the network, including accuracy and confusion matrix\n",
        "\n",
        "Are the results better than the previous section?\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9WyUlmps5sO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIpmlkOdyRaS"
      },
      "source": [
        "Finally show 3 reviews from the test data with correct labales and 3 without, why do you think the network did not success on these examples?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LLdBU7zSyQZa"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
